from transformers import AutoModel, AutoTokenizer
from sentence_transformers import SentenceTransformer, models
import os
import streamlit as st
from langchain_chroma import Chroma
from sentence_transformers import CrossEncoder

def load_db(embedding_function):
    return Chroma(persist_directory="dblast1.db",embedding_function=embedding_function)

def augment_multiple_query(query,llm):
    messages = [
        {
            "role": "system",
            "content": "ã‚ãªãŸã¯å½¹ã«ç«‹ã¤å°‚é–€å®¶ã®æ ¡å‰‡èª¿æŸ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æ ¡å‰‡ã«ã¤ã„ã¦è³ªå•ã—ã¦ã„ã¾ã™ã€‚"
            "æä¾›ã•ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦ã€å½¼ã‚‰ãŒå¿…è¦ãªæƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‹ã®ã‚’åŠ©ã‘ã‚‹ãŸã‚ã«ã€å¿…ãš3ã¤ä»¥ä¸Š5ã¤ä»¥ä¸‹ã®è¿½åŠ ã®é–¢é€£ã™ã‚‹è³ªå•ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
            "è¤‡æ–‡ã®ãªã„çŸ­ã„è³ªå•ã ã‘ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚ãƒˆãƒ”ãƒƒã‚¯ã®ã•ã¾ã–ã¾ãªå´é¢ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ã•ã¾ã–ã¾ãªè³ªå•ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
            "å®Œå…¨ãªè³ªå•ã§ã‚ã‚‹ã“ã¨ã€å…ƒã®è³ªå•ã«é–¢é€£ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            "1è¡Œã«1ã¤ã®è³ªå•ã‚’å‡ºåŠ›ã™ã‚‹ã€‚è³ªå•ã«ç•ªå·ã‚’ã¤ã‘ãªã„ã§ãã ã•ã„ã€‚"
        },
        {"role": "user", "content": query}
    ]
    response = llm.invoke(
        input=messages,
    )
    content = response
    content = content.split("\n")
    return content 

def rag_c(question,context,llm):
    c=f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>ã¾ãšã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã‚¯ã‚¨ãƒªã‚’è¦‹ã¦ã€ãã®ã‚¯ã‚¨ãƒªãŒæŒ¨æ‹¶ã€æ„Ÿè¬ã®è¨€è‘‰ã€è¬ç½ªã®è¨€è‘‰ã‚„è³ªå•ã—ãŸã„æ°—æŒã¡ã‚’è¡¨ã™å†…å®¹ã§ã‚ã‚Œã°ã€é©åˆ‡ãªè¨€è‘‰ã§è¿”ã™ã—ã¦ãã ã•ã„ã€‚å‚è€ƒæƒ…å ±é–¢é€£ã®è©±ã‚’å…¥ã‚Œãªã„ã§ãã ã•ã„ã€‚
                            ãã†ã§ã¯ãªã„å ´åˆã¯ã€å‚è€ƒæƒ…å ±ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«ã§ãã‚‹ã ã‘æ­£ç¢ºã‹ã¤å…·ä½“çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚ã§ãã‚‹ã ã‘å›ç­”ã«æ”¹è¡Œã‚’å…¥ã‚Œã¦å›ç­”ã‚’è¦‹ã‚„ã™ãã—ã¦ãã ã•ã„ã€‚
                            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã¨é–¢é€£ã™ã‚‹é©åˆ‡ãªæƒ…å ±ãŒå‚è€ƒæƒ…å ±ã«ãªã„å ´åˆã¯ã€"é©åˆ‡ãªæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"ã¨ç­”ãˆã¦ãã ã•ã„ã€‚
                            ãªãŠã€ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã‚¯ã‚¨ãƒªãŒæŒ¨æ‹¶ã€æ„Ÿè¬ã®è¨€è‘‰ã€è¬ç½ªã®è¨€è‘‰ã‚„è³ªå•ã—ãŸã„æ°—æŒã¡ã‚’è¡¨ã™å†…å®¹ã§ãªã„å ´åˆã€å‚è€ƒæƒ…å ±ã«åŸºã¥ã„ã¦æ­£ç¢ºã‹ã¤è©³ç´°ã«å›ç­”ã—ã¾ã™ã€‚ã€ã¨ã„ã£ãŸå†…å®¹ã¯å›ç­”ã«å…¥ã‚Œãªã„ã§ãã ã•ã„ã€‚
    <|eot_id|><|start_header_id|>user<|end_header_id|>
    {context}\nè³ªå•: {question} \n<|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """
    result = llm.invoke(c).split("assistant<|end_header_id|>")[-1].strip()
    return result

def init_page():
    st.set_page_config(
        page_title='ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ',
        page_icon='ğŸ§‘â€ğŸ’»',
    )
def qa(original_query,db,llm):
    # cross_encoder = CrossEncoder('cross_rank')
    MODEL_NAME = "hotchpotch/japanese-reranker-cross-encoder-large-v1"
    cross_encoder = CrossEncoder(MODEL_NAME)
    augmented_queries = augment_multiple_query(original_query,llm)
    augmented_queries = [augmented_querie.strip() for augmented_querie in augmented_queries if augmented_querie.strip()]
    augmented_queries.append(original_query)
    results=[]
    for augmented_query in augmented_queries:
        result = db.similarity_search(augmented_query,k=3,)
        results.append(result)
    # Deduplicate the retrieved documents
    unique_documents = set()
    for documents in results:
        for document in documents:
            unique_documents.add(document.page_content)
    unique_documents = list(unique_documents)
    pairs = []
    for doc in unique_documents:
        pairs.append([original_query, doc])
    scores = cross_encoder.predict(pairs)
    scored_docs = zip(scores, unique_documents)
    sorted_docs = sorted(scored_docs, reverse=True)
    reranked_docs = [doc for _, doc in sorted_docs][0:10]
    context1=''
    for i in range(0, 3): 
        context1+=reranked_docs[i]
        context1+="\n\n"
    return rag_c(original_query,context1,llm), context1, reranked_docs

def main():
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # # ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
    # model_path = '/home/RAG/model'

    # try:
    #     # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰
    #     tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
    #     model = AutoModel.from_pretrained(model_path, local_files_only=True)

    #     # SentenceTransformerç”¨ã«ãƒ©ãƒƒãƒ—
    #     transformer_model = models.Transformer(model_path)
    #     pooling_model = models.Pooling(transformer_model.get_word_embedding_dimension())
    #     model = SentenceTransformer(modules=[transformer_model, pooling_model])   

    #     # åŸ‹ã‚è¾¼ã¿é–¢æ•°ã‚’å®šç¾©
    #     class SentenceTransformerEmbeddingFunction:
    #         def __init__(self, model):
    #             self.model = model
            
    #         def embed_documents(self, texts):
    #             return self.model.encode(texts).tolist()
            
    #         def embed_query(self, text):
    #             return self.model.encode([text])[0].tolist()

    #     # åŸ‹ã‚è¾¼ã¿é–¢æ•°ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    #     embedding_function = SentenceTransformerEmbeddingFunction(model)

    # except Exception as e:
    #     print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    embedding_function = SentenceTransformer("intfloat/multilingual-e5-large")
    db = load_db(embedding_function)
    init_page()

    from langchain_ollama import OllamaLLM
    local_llm1 = OllamaLLM(base_url="http://csink12.nda.ac.jp:11434",
        model="elyza:70b",
        temperature=0.01
    )
    local_llm2 = OllamaLLM(base_url="http://csink12.nda.ac.jp:11434",
        model="elyza:70b",
        temperature=0
    )

    if "messages" not in st.session_state:
      st.session_state.messages = []
    if user_input := st.chat_input('è³ªå•ã—ã‚ˆã†ï¼'):
        # ä»¥å‰ã®ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¡¨ç¤º
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        print(user_input)
        with st.chat_message('user'):
            st.markdown(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message('assistant'):
            with st.spinner('é˜²å¤§ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒæ¤œç´¢ä¸­ ...'):
                response = qa(user_input,db,local_llm2)
                st.markdown(response[0])
                print(response[0])
                st.markdown("\n\n")
                st.markdown(f"å‚è€ƒå…ƒï¼š{response[2][0]}")
        st.session_state.messages.append({"role": "assistant", "content": response[0]})

if __name__ == "__main__":
  main()
